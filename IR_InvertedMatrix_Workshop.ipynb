{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "890288b1",
   "metadata": {},
   "source": [
    "# 🛠️ Active Learning Workshop: Implementing an Inverted Matrix (Jupyter + GitHub Edition)\n",
    "## 🔍 Workshop Theme\n",
    "*Readable, correct, and collaboratively reviewed code—just like in the real world.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e78d18",
   "metadata": {},
   "source": [
    "\n",
    "Welcome to the 90-minute workshop! In this hands-on session, your team will build an **Inverted Index** pipeline, the foundation of many intelligent systems that need fast and relevant access to text data — such as AI agents.\n",
    "\n",
    "### 👥 Team Guidelines\n",
    "- Work in teams of 3.\n",
    "- Submit one completed Jupyter Notebook per team.\n",
    "- The final notebook must contain **Markdown explanations** and **Python code**.\n",
    "- Push your notebook to GitHub and share the `.git` link before class ends.\n",
    "\n",
    "---\n",
    "## 🔧 Workshop Tasks Overview\n",
    "\n",
    "1. **Document Collection**\n",
    "2. **Tokenizer Implementation**\n",
    "3. **Normalization Pipeline (Stemming, Stop Words, etc.)**\n",
    "4. **Build and Query the Inverted Index**\n",
    "\n",
    "> Each step includes a sample **talking point**. Your team must add your own custom **Markdown + code cells** with a **second talking point**, and test your Inverted Index with **2 phrase queries**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a922333",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 🧠 Learning Objectives\n",
    "- Implement an **Inverted Matrix** using real-world data during the NLP process.\n",
    "- Build **Jupyter Notebooks** with well-structured code and clear Markdown documentation.\n",
    "- Use **Git and GitHub** for collaborative version control and code sharing.\n",
    "- Identify and articulate coding issues (\"**talking points**\") and insert them directly into peer notebooks.\n",
    "- Practice **collaborative debugging**, professional peer feedback, and improve code quality.\n",
    "\n",
    "## 🧩 Workshop Structure (90 Minutes)\n",
    "1. **Instructor Use Case Introduction** *(15 min)* – Set up teams of 3 people. Read and understand the workshop, plus submission instructions. Seek assistance if needed.\n",
    "2. **Team Jupyter Notebook Development** *(45 min)* – Manual IR and Inverted Matrix coding + Markdown documentation (work as teams)\n",
    "3. **Push to GitHub** *(15 min)* – Teams commit and push initial notebooks. **Make sure to include your names so it is easy to identify the team that developed the Min-Max code**.\n",
    "4. **Instructor Review** - The instructor will go around, take notes, and provide coaching as needed, during the **Peer Review Round**\n",
    "5. **Email Delivery** *(15 min)* – Each team send the instructor an email **with the *.git link** to the GitHub repo **(one email/team)**. Subject on the email is: PROG8245 - Inverted Matrix  Workshop, Team #_____.\n",
    "\n",
    "\n",
    "## 💻 Submission Checklist\n",
    "- ✅ `IR_InvertedMatrix_Workshop.ipynb` with:\n",
    "  - Demo code: Document Collection, Tokenizer, Normalization Pipeline, and Inverted Index.\n",
    "  - Markdown explanations for each major step\n",
    "  - **Labeled talking point(s)** and 2 phrase query tests\n",
    "- ✅ `README.md` with:\n",
    "  - Dataset description\n",
    "  - Team member names\n",
    "  - Link to the dataset and license (if public)\n",
    "- ✅ GitHub Repo:\n",
    "  - Public repo named `IR-invertedmatrix-workshop`\n",
    "  - This is a group effort, so **choose one member of the team** to publish the repo\n",
    "  - At least **one commit containing one meaningful talking point**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e160c9d",
   "metadata": {},
   "source": [
    "## 📄 Step 1: Document Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc964464",
   "metadata": {},
   "source": [
    "\n",
    "### 🗣 Instructor Talking Point:\n",
    "> We begin by gathering a text corpus. To build a robust index, your vocabulary should include **over 2000 unique words**. You can use scraped articles, academic papers, or open datasets.\n",
    "\n",
    "### 🔧 Your Task:\n",
    "- Collect at least 20+ text documents.\n",
    "- Ensure the vocabulary exceeds 2000 unique words.\n",
    "- Load the documents into a list for processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae4f8d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "377bb667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 28 documents.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to data\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package reuters to data\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\parth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "from nltk.corpus import reuters\n",
    "from nltk.stem import PorterStemmer\n",
    "import os\n",
    "import nltk\n",
    "DATA_DIR = os.path.join(os.path.dirname('data'), 'data', 'nltk_data')\n",
    "\n",
    "# 2. Make sure it exists\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "# 3. Tell NLTK to look in there\n",
    "nltk.data.path.append(DATA_DIR)\n",
    "\n",
    "# 4. Download the Gutenberg corpus into that folder\n",
    "nltk.download('gutenberg', download_dir=DATA_DIR)\n",
    "nltk.download('reuters', download_dir=DATA_DIR)\n",
    "\n",
    "\n",
    "\n",
    "# List of file IDs (there are 18 built-in Gutenberg books)\n",
    "nltk.download('gutenberg')   \n",
    "file_ids = gutenberg.fileids()\n",
    "documents = [gutenberg.raw(file_id) for file_id in file_ids]\n",
    "\n",
    "# If you want 20+ documents, you can duplicate or supplement from other corpora\n",
    "\n",
    "\n",
    "\n",
    "# Add 10 more from Reuters\n",
    "reuters_ids = reuters.fileids()[:10]\n",
    "documents.extend([reuters.raw(doc_id) for doc_id in reuters_ids])\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b342945a",
   "metadata": {},
   "source": [
    "## ✂️ Step 2: Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2803fb52",
   "metadata": {},
   "source": [
    "\n",
    "### 🗣 Instructor Talking Point:\n",
    "> The tokenizer breaks raw text into a stream of words (tokens). This is the foundation for every later step in IR and NLP.\n",
    "\n",
    "### 🔧 Your Task:\n",
    "- Implement a basic tokenizer that splits text into lowercase words.\n",
    "- Handle punctuation removal and basic non-alphanumeric filtering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bfecb7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized: austen-emma.txt — 161983 tokens\n",
      "Tokenized: austen-persuasion.txt — 84167 tokens\n",
      "Tokenized: austen-sense.txt — 120787 tokens\n",
      "Tokenized: bible-kjv.txt — 854046 tokens\n",
      "Tokenized: blake-poems.txt — 6936 tokens\n",
      "Tokenized: bryant-stories.txt — 46703 tokens\n",
      "Tokenized: burgess-busterbrown.txt — 16363 tokens\n",
      "Tokenized: carroll-alice.txt — 27336 tokens\n",
      "Tokenized: cats.txt — 36329 tokens\n",
      "Tokenized: chesterton-ball.txt — 82867 tokens\n",
      "Tokenized: chesterton-brown.txt — 73288 tokens\n",
      "Tokenized: chesterton-thursday.txt — 58729 tokens\n",
      "Tokenized: edgeworth-parents.txt — 170796 tokens\n",
      "Tokenized: melville-moby_dick.txt — 218621 tokens\n",
      "Tokenized: milton-paradise.txt — 80497 tokens\n",
      "Tokenized: shakespeare-caesar.txt — 20873 tokens\n",
      "Tokenized: shakespeare-hamlet.txt — 30271 tokens\n",
      "Tokenized: shakespeare-macbeth.txt — 18351 tokens\n",
      "Tokenized: whitman-leaves.txt — 126605 tokens\n",
      "\n",
      "Preview from first document:\n",
      "['emma', 'by', 'jane', 'austen', '1816', 'volume', 'i', 'chapter', 'i', 'emma', 'woodhouse', 'handsome', 'clever', 'and', 'rich', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "# Tokenizer function\n",
    "def tokenize(text):\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    return tokens\n",
    "\n",
    "# Load and tokenize all .txt files from the 'data/' folder\n",
    "def load_and_tokenize_all(folder_path):\n",
    "    tokenized_documents = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8', errors= \"ignore\") as file:\n",
    "                text = file.read()\n",
    "                tokens = tokenize(text)\n",
    "                tokenized_documents.append(tokens)\n",
    "                print(f\"Tokenized: {filename} — {len(tokens)} tokens\")\n",
    "    return tokenized_documents\n",
    "\n",
    "# Run the function on your data folder\n",
    "folder_path = 'data/nltk_data/corpora'\n",
    "all_tokenized_docs = load_and_tokenize_all(folder_path)\n",
    "\n",
    "# Preview the first 30 tokens from the first document\n",
    "print(\"\\nPreview from first document:\")\n",
    "print(all_tokenized_docs[0][:30])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ed76ec",
   "metadata": {},
   "source": [
    "## 🔁 Step 3: Normalization Pipeline (Stemming, Stop Word Removal, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f277a0d",
   "metadata": {},
   "source": [
    "\n",
    "### 🗣 Instructor Talking Point:\n",
    "> Now we normalize tokens: convert to lowercase, remove stop words, apply stemming or affix stripping. This reduces redundancy and enhances search accuracy.\n",
    "\n",
    "### 🔧 Your Task:\n",
    "- Use `nltk` to remove stopwords and apply stemming.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "580ccb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: austen-emma.txt — 73532 tokens (stopwords removed & stemmed)\n",
      "Processed: austen-persuasion.txt — 38383 tokens (stopwords removed & stemmed)\n",
      "Processed: austen-sense.txt — 54040 tokens (stopwords removed & stemmed)\n",
      "Processed: bible-kjv.txt — 437149 tokens (stopwords removed & stemmed)\n",
      "Processed: blake-poems.txt — 3807 tokens (stopwords removed & stemmed)\n",
      "Processed: bryant-stories.txt — 21810 tokens (stopwords removed & stemmed)\n",
      "Processed: burgess-busterbrown.txt — 7618 tokens (stopwords removed & stemmed)\n",
      "Processed: carroll-alice.txt — 12243 tokens (stopwords removed & stemmed)\n",
      "Processed: cats.txt — 36329 tokens (stopwords removed & stemmed)\n",
      "Processed: chesterton-ball.txt — 39900 tokens (stopwords removed & stemmed)\n",
      "Processed: chesterton-brown.txt — 35350 tokens (stopwords removed & stemmed)\n",
      "Processed: chesterton-thursday.txt — 28333 tokens (stopwords removed & stemmed)\n",
      "Processed: edgeworth-parents.txt — 78207 tokens (stopwords removed & stemmed)\n",
      "Processed: melville-moby_dick.txt — 110719 tokens (stopwords removed & stemmed)\n",
      "Processed: milton-paradise.txt — 45572 tokens (stopwords removed & stemmed)\n",
      "Processed: shakespeare-caesar.txt — 11127 tokens (stopwords removed & stemmed)\n",
      "Processed: shakespeare-hamlet.txt — 15903 tokens (stopwords removed & stemmed)\n",
      "Processed: shakespeare-macbeth.txt — 10157 tokens (stopwords removed & stemmed)\n",
      "Processed: whitman-leaves.txt — 65409 tokens (stopwords removed & stemmed)\n",
      "\n",
      "Preview from first document:\n",
      "['emma', 'jane', 'austen', '1816', 'volum', 'chapter', 'emma', 'woodhous', 'handsom', 'clever', 'rich', 'comfort', 'home', 'happi', 'disposit', 'seem', 'unit', 'best', 'bless', 'exist', 'live', 'nearli', 'twenti', 'one', 'year', 'world', 'littl', 'distress', 'vex', 'youngest']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "STEMMER = PorterStemmer()\n",
    "\n",
    "\n",
    "def normalize(tokens):\n",
    "    normalized = []\n",
    "    for t in tokens:\n",
    "        if t in STOPWORDS:\n",
    "            continue\n",
    "        stemmed = STEMMER.stem(t)\n",
    "        normalized.append(stemmed)\n",
    "    return normalized\n",
    "\n",
    "# Load, tokenize, normalize all .txt files\n",
    "def load_and_process_all(folder_path):\n",
    "    all_docs = []\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for fname in files:\n",
    "            if not fname.endswith('.txt'):\n",
    "                continue\n",
    "            path = os.path.join(root, fname)\n",
    "            with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                text = f.read()\n",
    "            tokens = tokenize(text)\n",
    "            tokens = normalize(tokens)\n",
    "            all_docs.append(tokens)\n",
    "            print(f\"Processed: {fname} — {len(tokens)} tokens (stopwords removed & stemmed)\")\n",
    "    return all_docs\n",
    "\n",
    "# Run it\n",
    "folder_path = 'data/nltk_data/corpora'\n",
    "all_tokenized_docs = load_and_process_all(folder_path)\n",
    "\n",
    "# Preview the first 30 normalized tokens from the first document\n",
    "print(\"\\nPreview from first document:\")\n",
    "print(all_tokenized_docs[0][:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a34cf58",
   "metadata": {},
   "source": [
    "## 🔍 Step 4: Inverted Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847c39dd",
   "metadata": {},
   "source": [
    "\n",
    "### 🗣 Instructor Talking Point:\n",
    "> We now map each normalized token to the list of document IDs in which it appears. This is the core structure that allows fast Boolean and phrase queries.\n",
    "\n",
    "### 🔧 Your Task:\n",
    "- Build the inverted index using a dictionary.\n",
    "- Add code to support phrase queries using positional indexing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9fb8fea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'emma': [0, 1], 'jane': [0, 1, 2], 'austen': [0, 1, 2], '1816': [0], 'volum': [0, 1, 2, 3, 8, 9, 10, 11, 12, 14, 15, 16, 17], 'chapter': [0, 1, 2, 7, 8, 10, 11, 12], 'woodhous': [0], 'handsom': [0, 1, 2, 5, 7, 8, 9, 10, 11, 12, 17], 'clever': [0, 1, 2, 5, 7, 8, 9, 10, 11, 12], 'rich': [0, 1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def build_inverted_index(documents):\n",
    "    index = defaultdict(list)\n",
    "    for doc_id, text in enumerate(documents):\n",
    "        tokens = normalize(tokenize(text))\n",
    "        seen = set()\n",
    "        for token in tokens:\n",
    "            if token not in seen:\n",
    "                index[token].append(doc_id)\n",
    "                seen.add(token)\n",
    "    return index\n",
    "\n",
    "inverted_index = build_inverted_index(documents)\n",
    "print(dict(list(inverted_index.items())[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faef4df8",
   "metadata": {},
   "source": [
    "## 🧪 Test: Phrase Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db832216",
   "metadata": {},
   "source": [
    "\n",
    "### 🗣 Instructor Talking Point:\n",
    "> A phrase query requires the exact sequence of terms (e.g., \"machine learning\"). To support this, extend the inverted index to store positions, not just docIDs.\n",
    "\n",
    "### 🔧 Your Task:\n",
    "- Implement 2 phrase queries.\n",
    "- Demonstrate that they return the correct documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "97132fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for phrase query implementation\n",
    "# You may build a position-aware index or use string search within docs after normalization\n",
    "\n",
    "# Example:\n",
    "query1 = \"machine learning\"\n",
    "query2 = \"deep model\"\n",
    "\n",
    "# Your implementation here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6065264",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "\n",
    "def build_positional_index(documents):\n",
    "    index = defaultdict(lambda: defaultdict(list))\n",
    "    for doc_id, text in enumerate(documents):\n",
    "        tokens = normalize(tokenize(text))\n",
    "        for pos, token in enumerate(tokens):\n",
    "            index[token][doc_id].append(pos)\n",
    "    return index\n",
    "\n",
    "# Build positional index\n",
    "positional_index = build_positional_index(documents)\n",
    "\n",
    "# Preview first 5 terms\n",
    "for term in list(positional_index.keys())[:30]:\n",
    "    print(f\"{term}: {dict(positional_index[term])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "67e85476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def phrase_query(phrase, index):\n",
    "    words = normalize(tokenize(phrase))\n",
    "    if not words:\n",
    "        return set()\n",
    "\n",
    "    # Start with the docIDs of the first word\n",
    "    candidate_docs = set(index[words[0]].keys())\n",
    "\n",
    "    for word in words[1:]:\n",
    "        candidate_docs &= set(index[word].keys())\n",
    "\n",
    "    matched_docs = set()\n",
    "\n",
    "    for doc_id in candidate_docs:\n",
    "        positions_lists = [index[word][doc_id] for word in words]\n",
    "        \n",
    "        # Check if there's a sequence of positions that are consecutive\n",
    "        for start_pos in positions_lists[0]:\n",
    "            if all((start_pos + i) in positions_lists[i] for i in range(1, len(words))):\n",
    "                matched_docs.add(doc_id)\n",
    "                break\n",
    "\n",
    "    return matched_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4c1038a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for phrase query: \"rich\"\n",
      "- Document #0: austen-emma.txt\n",
      "- Document #1: austen-persuasion.txt\n",
      "- Document #2: austen-sense.txt\n",
      "- Document #3: bible-kjv.txt\n",
      "- Document #4: blake-poems.txt\n",
      "- Document #5: bryant-stories.txt\n",
      "- Document #7: carroll-alice.txt\n",
      "- Document #8: chesterton-ball.txt\n",
      "- Document #9: chesterton-brown.txt\n",
      "- Document #10: chesterton-thursday.txt\n",
      "- Document #11: edgeworth-parents.txt\n",
      "- Document #12: melville-moby_dick.txt\n",
      "- Document #13: milton-paradise.txt\n",
      "- Document #14: shakespeare-caesar.txt\n",
      "- Document #15: shakespeare-hamlet.txt\n",
      "- Document #16: shakespeare-macbeth.txt\n",
      "- Document #17: whitman-leaves.txt\n",
      "\n",
      "Results for phrase query: \"seemed to unite some of the best blessings\"\n",
      "- Document #0: austen-emma.txt\n"
     ]
    }
   ],
   "source": [
    "query1 = \"rich\"\n",
    "query2 = \"seemed to unite some of the best blessings\"\n",
    "\n",
    "results1 = phrase_query(query1, positional_index)\n",
    "results2 = phrase_query(query2, positional_index)\n",
    "\n",
    "print(f\"\\nResults for phrase query: \\\"{query1}\\\"\")\n",
    "for doc_id in results1:\n",
    "    print(f\"- Document #{doc_id}: {file_ids[doc_id] if doc_id < len(file_ids) else 'Reuters'}\")\n",
    "\n",
    "print(f\"\\nResults for phrase query: \\\"{query2}\\\"\")\n",
    "for doc_id in results2:\n",
    "    print(f\"- Document #{doc_id}: {file_ids[doc_id] if doc_id < len(file_ids) else 'Reuters'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caaf4c70",
   "metadata": {},
   "source": [
    "### Parth:\n",
    "> To enable accurate phrase querying, we built a **positional index** instead of a basic inverted index. This allows us to not only find documents containing the keywords, but also verify that the keywords appear **in the exact order and adjacent positions**. Without this structure, phrases like `\"seemed to unite\"` could yield false positives.\n",
    "\n",
    "### Adithya:\n",
    "> Our `phrase_query()` function cross-checks position lists to ensure word sequences are **adjacent** (e.g., if word1 appears at index 4, word2 must appear at index 5, etc.). We confirmed its correctness by testing with two phrase queries that vary in length and complexity. This replicates how search engines handle multi-word queries.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
